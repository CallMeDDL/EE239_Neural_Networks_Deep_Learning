{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_project_slidingW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dPk0rIMsp-MS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# considering change label to one-hot encoding\n",
        "# list data as array described as graph\n",
        "# date set is balanced\n",
        "# sliding window hyperparameters waiting to be tuned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1i9CZdD_qFmE",
        "colab_type": "code",
        "outputId": "e7608a6c-2687-471c-c5b8-b4449a22ce75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "# considering change label to one-hot encoding\n",
        "# list data as array described as graph\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WM6rnkEwqYqR",
        "colab_type": "code",
        "outputId": "1f0b6ea4-6740-43a2-87d9-d795ab1a3ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "import os\n",
        "os.chdir(\"drive/Colab Notebooks\") \n",
        "!ls"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
            " 1-1_tensor_tutorial.ipynb\t\t  'RNN_project (58c845cd).ipynb'\n",
            " 1-2_autograd_tutorial.ipynb\t\t   RNN_project.ipynb\n",
            "'1-3_neural_networks_tutorial (1).ipynb'   RNN_project_slidingW.ipynb\n",
            "'1-4_cifar10_tutorial (1).ipynb'\t  'RNN_project_slidingW _olld.ipynb'\n",
            " 1_hello_tensorflow.ipynb\t\t   RNN_standardize_sliding_window.ipynb\n",
            " 2_getting_started.ipynb\t\t   RNN_Vanilla.ipynb\n",
            "'3_mnist_from_scratch (6f53deeb).ipynb'    Untitled\n",
            " 3_mnist_from_scratch.ipynb\t\t   Untitled0.ipynb\n",
            "'AE(just one model).ipynb'\t\t   Untitled1.ipynb\n",
            " data\t\t\t\t\t   Untitled2.ipynb\n",
            " drive\t\t\t\t\t   Untitled3.ipynb\n",
            " ee239_1000epoch.ipynb\t\t\t   Untitled4.ipynb\n",
            " EEG_AE.ipynb\t\t\t\t   Untitled5.ipynb\n",
            " HW5_CNN.ipynb\t\t\t\t   Untitled6.ipynb\n",
            " record.txt\t\t\t\t   Untitled7.ipynb\n",
            " record.txt_without_weights\t\t   Untitled8.ipynb\n",
            " record_without_weights.txt\t\t  'Untitled (93cd11de)'\n",
            " results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sWnEQRkhp-MW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# library import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from time import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ym5sRIiRp-Ma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define hyperparameters and functions\n",
        "batch_size = 100\n",
        "num_epoches = 100\n",
        "\n",
        "window_step = 5\n",
        "window_length = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "betas=(0.9, 0.999)\n",
        "weight_decay=5e-4\n",
        "\n",
        "\n",
        "index = np.arange(2115)\n",
        "np.random.shuffle(index)\n",
        "training_index, valid_index = index[:1715], index[1715:]\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "\n",
        "use_gpu = torch.cuda.is_available()  # determine whether it has gpu speed up\n",
        "\n",
        "\n",
        "def plot_acc(train_acc, test_acc):\n",
        "    plt.plot(test_acc, label = \"test\")\n",
        "    plt.plot(train_acc, label = \"train\")\n",
        "    plt.xlabel(\"eopch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(\"The accuracy against epoch\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def flatten(x):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - Tensor of shape (N, D1, ..., DM)\n",
        "    Output:\n",
        "    - Tensor of shape (N, D1 * ... * DM)\n",
        "    \"\"\"\n",
        "    x_shape = x.size()\n",
        "    new_shape = 1\n",
        "    for i in range(len(x_shape) - 1):\n",
        "        new_shape *= x_shape[i + 1]\n",
        "    x_flat = x.reshape((x_shape[0],new_shape))\n",
        "    return x_flat\n",
        "  \n",
        "def sliding_window(length, shift, data):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - raw eeg data\n",
        "    Output:\n",
        "    - processed window signals, everytime shifted with \"shift\" units, window length is \"length\"\n",
        "    \"\"\"\n",
        "    p, e, d = data.shape\n",
        "    \n",
        "    if(d%shift==0):\n",
        "        new_data_size = d//shift\n",
        "        new_data = np.zeros((p, e, new_data_size))\n",
        "        for i in range(d//shift):\n",
        "            new_data[:, :, i] = np.mean(data[:, :, i*shift:(i+1)*shift+length], axis = 2)\n",
        "\n",
        "    else:\n",
        "        new_data_size = d//shift+1\n",
        "        new_data = np.zeros((p, e, new_data_size))\n",
        "        for i in range(d//shift):\n",
        "            new_data[:, :, i] = np.mean(data[:, :, i*shift:(i+1)*shift+length], axis = 2)\n",
        "        new_data[:, :, new_data_size-1] = np.mean(data[:, :, (new_data_size-1)*shift:], axis = 2)\n",
        "    \n",
        "    return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGLOZRRyp-Mc",
        "colab_type": "code",
        "outputId": "f8b58ced-6e80-485f-d483-f71b3c062173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# load data into environment\n",
        "X_test = np.load(\"data/X_test.npy\")\n",
        "y_test = np.load(\"data/y_test.npy\")\n",
        "person_train_valid = np.load(\"data/person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"data/X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"data/y_train_valid.npy\")\n",
        "person_test = np.load(\"data/person_test.npy\")\n",
        "\n",
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/Valid data shape: (2115, 25, 1000)\n",
            "Test data shape: (443, 25, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cYObb2o0p-Mi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# transform data into pytorch form\n",
        "# extract first 22 signals out of 25 signals\n",
        "X_train_valid = X_train_valid[:, :22, :]\n",
        "X_train = X_train_valid[training_index]\n",
        "X_valid = X_train_valid[valid_index]\n",
        "X_test = X_test[:, :22, :]\n",
        "\n",
        "# standardize data\n",
        "X_train = X_train.reshape((1715, 22*1000))\n",
        "X_valid = X_valid.reshape((400, 22*1000))\n",
        "X_test = X_test.reshape((443, 22*1000))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = X_train.reshape((1715, 22, 1000))\n",
        "X_valid = X_valid.reshape((400, 22, 1000))\n",
        "X_test = X_test.reshape((443, 22, 1000))\n",
        "\n",
        "# sliding window process\n",
        "X_train = sliding_window(window_length, window_step, X_train)\n",
        "X_valid = sliding_window(window_length, window_step, X_valid)\n",
        "X_test = sliding_window(window_length, window_step, X_test)\n",
        "\n",
        "\n",
        "y_train = y_train_valid[training_index]\n",
        "y_valid = y_train_valid[valid_index]\n",
        "\n",
        "\n",
        "trainset = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train - 769).long(), torch.from_numpy(person_train_valid[training_index]))\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "validset = torch.utils.data.TensorDataset(torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid - 769).long())\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "testset = torch.utils.data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test - 769).long())\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZiOPeeeJp-Ml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# network architecture\n",
        "class eeg_lstm(nn.Module):\n",
        "     def __init__(self, in_feature=22, hidden_feature=256, num_class=4, signal_length=1000):\n",
        "          super(eeg_lstm, self).__init__()\n",
        "          self.rnn = nn.LSTM(input_size=in_feature, hidden_size=hidden_feature, num_layers=2, bidirectional=True) #using 5 layers lstm\n",
        "          self.relu = nn.ReLU()\n",
        "          self.dropout = nn.Dropout(0.75)\n",
        "          self.classifier = nn.Linear(hidden_feature*signal_length*2, num_class) #using linear to classify the output\n",
        "          \n",
        "     def forward(self, x):\n",
        "          out, _ = self.rnn(x) #使用默认的隐藏状态，得到的out是（1000， batch， hidden_feature）\n",
        "          #print(out.shape)\n",
        "          out = self.dropout(out)\n",
        "          out = self.relu(out)\n",
        "          out = self.dropout(out)\n",
        "          #out = out[-101:-1,:,:]#取序列中的最后一个，大小是（batch， hidden_feature)\n",
        "          out = flatten(out.permute(1, 0, 2))\n",
        "          #print(out.shape)\n",
        "          out = self.classifier(out) #得到分类结果\n",
        "          #tag_score = F.log_softmax(out, dim=1)\n",
        "          return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XY2M6QEOp-Mo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initiate lstm network\n",
        "model = eeg_lstm(signal_length=X_train.shape[2])  \n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "\n",
        "\n",
        "# define loss function and optimizer\n",
        "weights = [0.23, 0.34, 0.25, 0.18]\n",
        "class_weights = torch.FloatTensor(weights).cuda()\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# parameters in Adam could be tuned\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOfuf4wwqlvl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  t0 = time()\n",
        "\n",
        "  \n",
        "  model.train()\n",
        "  print('epoch {}'.format(epoch + 1))\n",
        "  print('*' * 10)\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0\n",
        "  for batch_idx, (signals, tags, subjects) in enumerate(trainloader):\n",
        "      # Step 1. Remember that Pytorch accumulates gradients.\n",
        "      # We need to clear them out before each instance\n",
        "      model.zero_grad()\n",
        "\n",
        "      # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "      # Tensors of word indices.\n",
        "      signals = signals.squeeze(1)\n",
        "      signals = signals.permute(2, 0, 1)\n",
        "      if use_gpu:\n",
        "          signals = Variable(signals).cuda()\n",
        "          tags = Variable(tags).cuda()\n",
        "          subjects = Variable(subjects).cuda()\n",
        "      else:\n",
        "          signals = Variable(signals)\n",
        "          tags = Variable(tags)\n",
        "          subjects = Variable(subjects)\n",
        "\n",
        "      # Step 3. Run our forward pass.\n",
        "      tag_scores = model(signals)\n",
        "\n",
        "      # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "      #  calling optimizer.step()\n",
        "      loss = criterion(tag_scores, tags)\n",
        "      \n",
        "      for i in range(tags.size(0)):\n",
        "        tmp_score = tag_scores[i,:]\n",
        "        tmp_score = tmp_score.unsqueeze(0)\n",
        "#         print(tmp_score.size(0))\n",
        "#         print(tmp_score.size(1))\n",
        "\n",
        "        tmp_tag = torch.Tensor([tags[i]]).long()\n",
        "        if(use_gpu):\n",
        "          tmp_tag = Variable(tmp_tag).cuda()\n",
        "          tmp_score = Variable(tmp_score).cuda()\n",
        "        else:\n",
        "          tmp_tag = Variable(tmp_tag)\n",
        "          tmp_score = Variable(tmp_score).cuda()\n",
        "          \n",
        "        if(subjects[i]==1 or subjects[i]==2):\n",
        "          loss+=5.0*criterion(tmp_score, tmp_tag) / tags.size(0)\n",
        "        elif(subjects[i]==6):\n",
        "          loss+=3.0*criterion(tmp_score, tmp_tag) / tags.size(0)\n",
        "\n",
        "      \n",
        "      running_loss += loss.data.item() * tags.size(0)\n",
        "      _, pred = torch.max(tag_scores, 1)\n",
        "      num_correct = (pred == tags).sum()\n",
        "      running_acc += num_correct.data.item()\n",
        "      # 向后传播\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  train_acc.append(running_acc/(len(X_train)))\n",
        "  print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n",
        "      epoch + 1, running_loss / (len(X_train)), running_acc / (len(\n",
        "          X_train))))\n",
        "\n",
        "  model.eval()\n",
        "  eval_loss = 0.\n",
        "  eval_acc = 0.\n",
        "  for batch_idx, (signals, tags) in enumerate(validloader):\n",
        "\n",
        "      signals = signals.squeeze(1)\n",
        "      signals = signals.permute(2, 0, 1)\n",
        "      if use_gpu:\n",
        "          signals = Variable(signals).cuda()\n",
        "          tags = Variable(tags).cuda()\n",
        "      else:\n",
        "          signals = Variable(signals)\n",
        "          tags = Variable(tags)\n",
        "\n",
        "      tag_scores = model(signals)\n",
        "      loss = criterion(tag_scores, tags)\n",
        "      eval_loss += loss.data.item() * tags.size(0)\n",
        "      _, pred = torch.max(tag_scores, 1)\n",
        "      num_correct = (pred == tags).sum()\n",
        "      eval_acc += num_correct.data.item()\n",
        "  val_acc.append(eval_acc / (len(X_valid)))\n",
        "  print('Validation Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
        "      X_valid)), eval_acc / (len(X_valid))))\n",
        "  \n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vAcVVU6dDNnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_class(subject):\n",
        "  model.eval()\n",
        "  eval_loss = 0.\n",
        "  eval_acc = 0.\n",
        "  total = 0.\n",
        "  \n",
        "  for batch_idx, (signals, tags) in enumerate(testloader):\n",
        "    signals = signals.squeeze(1)\n",
        "    signals = signals.permute(2, 0, 1)\n",
        "    if use_gpu:\n",
        "        signals = Variable(signals).cuda()\n",
        "        tags = Variable(tags).cuda()\n",
        "    else:\n",
        "        signals = Variable(signals)\n",
        "        tags = Variable(tags)\n",
        "\n",
        "    tag_scores = model(signals)\n",
        "#     for i in range(9):\n",
        "#       tmp_index = tags.index(i)\n",
        "#       tmo_tag = tag\n",
        "    loss = criterion(tag_scores, tags)\n",
        "    #print(tags)\n",
        "    eval_loss += loss.data.item() * tags.size(0)\n",
        "    _, pred = torch.max(tag_scores, 1)\n",
        "    total += tags.size(0)\n",
        "    num_correct = (pred == tags).sum()\n",
        "    eval_acc += num_correct.data.item()\n",
        "  print('Class:{}, Testing Loss:{:.6f}, Acc:{:6f}.'.format(subject+1, eval_loss/len(testloader), eval_acc / total))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMHb8ZoSDXEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a4a7e23b-a06b-496f-a256-b501f475bde2"
      },
      "cell_type": "code",
      "source": [
        "x_test_list_class=[[] for i in range(4)]\n",
        "y_test_list_class=[[] for i in range(4)]\n",
        "for i in range(len(y_test)):\n",
        "  x_test_list_class[int(y_test[i])-769].append(X_test[i,:22,:])\n",
        "  y_test_list_class[int(y_test[i])-769].append(y_test[i])\n",
        "\n",
        "x_test_class=np.array(x_test_list_class)\n",
        "y_test_class=np.array(y_test_list_class)\n",
        "\n",
        "train_acc = []\n",
        "print(y_test_list_class)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769, 769], [770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770, 770], [771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771, 771], [772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772, 772]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jtFnm-kwE1mF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10557
        },
        "outputId": "ce22c290-e518-4dcc-aa28-451902b61032"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(num_epoches):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "  train(epoch)\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(\"#############################\")\n",
        "    for i in range(4):\n",
        "      testset = torch.utils.data.TensorDataset(torch.from_numpy(np.array(x_test_class[i])).float(), torch.from_numpy(np.array(y_test_class[i]) - 769).long())\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "      test_class(i)\n",
        "    testset = torch.utils.data.TensorDataset(torch.from_numpy(X_test[:,:22,:]).float(), torch.from_numpy(y_test - 769).long())\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_class(-1)\n",
        "    print(\"#############################\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "**********\n",
            "Finish 1 epoch, Loss: 3.389054, Acc: 0.281633\n",
            "Validation Loss: 1.320276, Acc: 0.340000\n",
            "\n",
            "epoch 2\n",
            "**********\n",
            "Finish 2 epoch, Loss: 3.238547, Acc: 0.345773\n",
            "Validation Loss: 1.260863, Acc: 0.477500\n",
            "\n",
            "epoch 3\n",
            "**********\n",
            "Finish 3 epoch, Loss: 3.070186, Acc: 0.402332\n",
            "Validation Loss: 1.172361, Acc: 0.497500\n",
            "\n",
            "epoch 4\n",
            "**********\n",
            "Finish 4 epoch, Loss: 2.964111, Acc: 0.467055\n",
            "Validation Loss: 1.233634, Acc: 0.365000\n",
            "\n",
            "epoch 5\n",
            "**********\n",
            "Finish 5 epoch, Loss: 2.833699, Acc: 0.495627\n",
            "Validation Loss: 1.101027, Acc: 0.545000\n",
            "\n",
            "epoch 6\n",
            "**********\n",
            "Finish 6 epoch, Loss: 2.716519, Acc: 0.514869\n",
            "Validation Loss: 1.053925, Acc: 0.555000\n",
            "\n",
            "epoch 7\n",
            "**********\n",
            "Finish 7 epoch, Loss: 2.628823, Acc: 0.539942\n",
            "Validation Loss: 1.045034, Acc: 0.550000\n",
            "\n",
            "epoch 8\n",
            "**********\n",
            "Finish 8 epoch, Loss: 2.599255, Acc: 0.544606\n",
            "Validation Loss: 1.034202, Acc: 0.560000\n",
            "\n",
            "epoch 9\n",
            "**********\n",
            "Finish 9 epoch, Loss: 2.531608, Acc: 0.544606\n",
            "Validation Loss: 1.032348, Acc: 0.577500\n",
            "\n",
            "epoch 10\n",
            "**********\n",
            "Finish 10 epoch, Loss: 2.494241, Acc: 0.583673\n",
            "Validation Loss: 1.092301, Acc: 0.500000\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:54.414560, Acc:0.567568.\n",
            "\n",
            "Class:2, Testing Loss:39.334397, Acc:0.834646.\n",
            "\n",
            "Class:3, Testing Loss:147.077682, Acc:0.333333.\n",
            "\n",
            "Class:4, Testing Loss:78.994986, Acc:0.348624.\n",
            "\n",
            "Class:0, Testing Loss:92.310568, Acc:0.539503.\n",
            "\n",
            "#############################\n",
            "epoch 11\n",
            "**********\n",
            "Finish 11 epoch, Loss: 2.429395, Acc: 0.574344\n",
            "Validation Loss: 1.059779, Acc: 0.562500\n",
            "\n",
            "epoch 12\n",
            "**********\n",
            "Finish 12 epoch, Loss: 2.302784, Acc: 0.608163\n",
            "Validation Loss: 0.993345, Acc: 0.590000\n",
            "\n",
            "epoch 13\n",
            "**********\n",
            "Finish 13 epoch, Loss: 2.259066, Acc: 0.610496\n",
            "Validation Loss: 1.045420, Acc: 0.550000\n",
            "\n",
            "epoch 14\n",
            "**********\n",
            "Finish 14 epoch, Loss: 2.300266, Acc: 0.625656\n",
            "Validation Loss: 0.973176, Acc: 0.605000\n",
            "\n",
            "epoch 15\n",
            "**********\n",
            "Finish 15 epoch, Loss: 2.229018, Acc: 0.619242\n",
            "Validation Loss: 1.039360, Acc: 0.545000\n",
            "\n",
            "epoch 16\n",
            "**********\n",
            "Finish 16 epoch, Loss: 2.146080, Acc: 0.637901\n",
            "Validation Loss: 0.961185, Acc: 0.597500\n",
            "\n",
            "epoch 17\n",
            "**********\n",
            "Finish 17 epoch, Loss: 2.024253, Acc: 0.665889\n",
            "Validation Loss: 0.974063, Acc: 0.585000\n",
            "\n",
            "epoch 18\n",
            "**********\n",
            "Finish 18 epoch, Loss: 2.002203, Acc: 0.669971\n",
            "Validation Loss: 0.960641, Acc: 0.597500\n",
            "\n",
            "epoch 19\n",
            "**********\n",
            "Finish 19 epoch, Loss: 2.019765, Acc: 0.660058\n",
            "Validation Loss: 0.981504, Acc: 0.605000\n",
            "\n",
            "epoch 20\n",
            "**********\n",
            "Finish 20 epoch, Loss: 2.063471, Acc: 0.655977\n",
            "Validation Loss: 1.003784, Acc: 0.587500\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:56.812795, Acc:0.612613.\n",
            "\n",
            "Class:2, Testing Loss:61.509461, Acc:0.574803.\n",
            "\n",
            "Class:3, Testing Loss:122.201626, Acc:0.479167.\n",
            "\n",
            "Class:4, Testing Loss:47.767388, Acc:0.678899.\n",
            "\n",
            "Class:0, Testing Loss:91.296478, Acc:0.589165.\n",
            "\n",
            "#############################\n",
            "epoch 21\n",
            "**********\n",
            "Finish 21 epoch, Loss: 1.985019, Acc: 0.674636\n",
            "Validation Loss: 1.009517, Acc: 0.582500\n",
            "\n",
            "epoch 22\n",
            "**********\n",
            "Finish 22 epoch, Loss: 1.967960, Acc: 0.656560\n",
            "Validation Loss: 0.998151, Acc: 0.585000\n",
            "\n",
            "epoch 23\n",
            "**********\n",
            "Finish 23 epoch, Loss: 1.925474, Acc: 0.682799\n",
            "Validation Loss: 1.020116, Acc: 0.592500\n",
            "\n",
            "epoch 24\n",
            "**********\n",
            "Finish 24 epoch, Loss: 1.921642, Acc: 0.686880\n",
            "Validation Loss: 0.993491, Acc: 0.605000\n",
            "\n",
            "epoch 25\n",
            "**********\n",
            "Finish 25 epoch, Loss: 1.770556, Acc: 0.710787\n",
            "Validation Loss: 0.985045, Acc: 0.620000\n",
            "\n",
            "epoch 26\n",
            "**********\n",
            "Finish 26 epoch, Loss: 1.835674, Acc: 0.706122\n",
            "Validation Loss: 1.000687, Acc: 0.590000\n",
            "\n",
            "epoch 27\n",
            "**********\n",
            "Finish 27 epoch, Loss: 1.794639, Acc: 0.706122\n",
            "Validation Loss: 0.948412, Acc: 0.585000\n",
            "\n",
            "epoch 28\n",
            "**********\n",
            "Finish 28 epoch, Loss: 1.685646, Acc: 0.729446\n",
            "Validation Loss: 1.032194, Acc: 0.602500\n",
            "\n",
            "epoch 29\n",
            "**********\n",
            "Finish 29 epoch, Loss: 1.768889, Acc: 0.721866\n",
            "Validation Loss: 0.992037, Acc: 0.622500\n",
            "\n",
            "epoch 30\n",
            "**********\n",
            "Finish 30 epoch, Loss: 1.682621, Acc: 0.723615\n",
            "Validation Loss: 1.058608, Acc: 0.602500\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:47.764318, Acc:0.666667.\n",
            "\n",
            "Class:2, Testing Loss:54.911896, Acc:0.669291.\n",
            "\n",
            "Class:3, Testing Loss:154.325420, Acc:0.458333.\n",
            "\n",
            "Class:4, Testing Loss:54.932187, Acc:0.587156.\n",
            "\n",
            "Class:0, Testing Loss:92.847902, Acc:0.602709.\n",
            "\n",
            "#############################\n",
            "epoch 31\n",
            "**********\n",
            "Finish 31 epoch, Loss: 1.661072, Acc: 0.739359\n",
            "Validation Loss: 1.017106, Acc: 0.615000\n",
            "\n",
            "epoch 32\n",
            "**********\n",
            "Finish 32 epoch, Loss: 1.541401, Acc: 0.751020\n",
            "Validation Loss: 0.983081, Acc: 0.635000\n",
            "\n",
            "epoch 33\n",
            "**********\n",
            "Finish 33 epoch, Loss: 1.632329, Acc: 0.753936\n",
            "Validation Loss: 0.973723, Acc: 0.617500\n",
            "\n",
            "epoch 34\n",
            "**********\n",
            "Finish 34 epoch, Loss: 1.538419, Acc: 0.752770\n",
            "Validation Loss: 0.987559, Acc: 0.607500\n",
            "\n",
            "epoch 35\n",
            "**********\n",
            "Finish 35 epoch, Loss: 1.552263, Acc: 0.756851\n",
            "Validation Loss: 1.047677, Acc: 0.587500\n",
            "\n",
            "epoch 36\n",
            "**********\n",
            "Finish 36 epoch, Loss: 1.533565, Acc: 0.756851\n",
            "Validation Loss: 1.022296, Acc: 0.610000\n",
            "\n",
            "epoch 37\n",
            "**********\n",
            "Finish 37 epoch, Loss: 1.445212, Acc: 0.758601\n",
            "Validation Loss: 1.073835, Acc: 0.617500\n",
            "\n",
            "epoch 38\n",
            "**********\n",
            "Finish 38 epoch, Loss: 1.408220, Acc: 0.767930\n",
            "Validation Loss: 1.008456, Acc: 0.607500\n",
            "\n",
            "epoch 39\n",
            "**********\n",
            "Finish 39 epoch, Loss: 1.447277, Acc: 0.768513\n",
            "Validation Loss: 1.093534, Acc: 0.615000\n",
            "\n",
            "epoch 40\n",
            "**********\n",
            "Finish 40 epoch, Loss: 1.450343, Acc: 0.770262\n",
            "Validation Loss: 0.998650, Acc: 0.617500\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:81.540587, Acc:0.504505.\n",
            "\n",
            "Class:2, Testing Loss:45.111823, Acc:0.740157.\n",
            "\n",
            "Class:3, Testing Loss:123.964771, Acc:0.562500.\n",
            "\n",
            "Class:4, Testing Loss:50.388832, Acc:0.633028.\n",
            "\n",
            "Class:0, Testing Loss:92.567543, Acc:0.616253.\n",
            "\n",
            "#############################\n",
            "epoch 41\n",
            "**********\n",
            "Finish 41 epoch, Loss: 1.530815, Acc: 0.772595\n",
            "Validation Loss: 1.011627, Acc: 0.635000\n",
            "\n",
            "epoch 42\n",
            "**********\n",
            "Finish 42 epoch, Loss: 1.375654, Acc: 0.780758\n",
            "Validation Loss: 1.024329, Acc: 0.600000\n",
            "\n",
            "epoch 43\n",
            "**********\n",
            "Finish 43 epoch, Loss: 1.409079, Acc: 0.784840\n",
            "Validation Loss: 1.041014, Acc: 0.625000\n",
            "\n",
            "epoch 44\n",
            "**********\n",
            "Finish 44 epoch, Loss: 1.389607, Acc: 0.802332\n",
            "Validation Loss: 0.981235, Acc: 0.627500\n",
            "\n",
            "epoch 45\n",
            "**********\n",
            "Finish 45 epoch, Loss: 1.255841, Acc: 0.801166\n",
            "Validation Loss: 1.072129, Acc: 0.607500\n",
            "\n",
            "epoch 46\n",
            "**********\n",
            "Finish 46 epoch, Loss: 1.258736, Acc: 0.802915\n",
            "Validation Loss: 1.002613, Acc: 0.635000\n",
            "\n",
            "epoch 47\n",
            "**********\n",
            "Finish 47 epoch, Loss: 1.296258, Acc: 0.796501\n",
            "Validation Loss: 1.023217, Acc: 0.620000\n",
            "\n",
            "epoch 48\n",
            "**********\n",
            "Finish 48 epoch, Loss: 1.238999, Acc: 0.817493\n",
            "Validation Loss: 1.044471, Acc: 0.635000\n",
            "\n",
            "epoch 49\n",
            "**********\n",
            "Finish 49 epoch, Loss: 1.164362, Acc: 0.819825\n",
            "Validation Loss: 1.097072, Acc: 0.617500\n",
            "\n",
            "epoch 50\n",
            "**********\n",
            "Finish 50 epoch, Loss: 1.253999, Acc: 0.816327\n",
            "Validation Loss: 1.031040, Acc: 0.645000\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:68.662818, Acc:0.567568.\n",
            "\n",
            "Class:2, Testing Loss:68.124675, Acc:0.574803.\n",
            "\n",
            "Class:3, Testing Loss:125.361866, Acc:0.562500.\n",
            "\n",
            "Class:4, Testing Loss:36.397342, Acc:0.752294.\n",
            "\n",
            "Class:0, Testing Loss:96.547989, Acc:0.613995.\n",
            "\n",
            "#############################\n",
            "epoch 51\n",
            "**********\n",
            "Finish 51 epoch, Loss: 1.150459, Acc: 0.823907\n",
            "Validation Loss: 1.049450, Acc: 0.627500\n",
            "\n",
            "epoch 52\n",
            "**********\n",
            "Finish 52 epoch, Loss: 1.088735, Acc: 0.840816\n",
            "Validation Loss: 1.095848, Acc: 0.625000\n",
            "\n",
            "epoch 53\n",
            "**********\n",
            "Finish 53 epoch, Loss: 1.071883, Acc: 0.833236\n",
            "Validation Loss: 1.040547, Acc: 0.630000\n",
            "\n",
            "epoch 54\n",
            "**********\n",
            "Finish 54 epoch, Loss: 1.034600, Acc: 0.858309\n",
            "Validation Loss: 1.111272, Acc: 0.625000\n",
            "\n",
            "epoch 55\n",
            "**********\n",
            "Finish 55 epoch, Loss: 1.032665, Acc: 0.838484\n",
            "Validation Loss: 1.113085, Acc: 0.650000\n",
            "\n",
            "epoch 56\n",
            "**********\n",
            "Finish 56 epoch, Loss: 1.125626, Acc: 0.830321\n",
            "Validation Loss: 1.144664, Acc: 0.602500\n",
            "\n",
            "epoch 57\n",
            "**********\n",
            "Finish 57 epoch, Loss: 1.103269, Acc: 0.832653\n",
            "Validation Loss: 1.089684, Acc: 0.620000\n",
            "\n",
            "epoch 58\n",
            "**********\n",
            "Finish 58 epoch, Loss: 0.998143, Acc: 0.859475\n",
            "Validation Loss: 1.050494, Acc: 0.645000\n",
            "\n",
            "epoch 59\n",
            "**********\n",
            "Finish 59 epoch, Loss: 0.913722, Acc: 0.861808\n",
            "Validation Loss: 1.133324, Acc: 0.635000\n",
            "\n",
            "epoch 60\n",
            "**********\n",
            "Finish 60 epoch, Loss: 1.037462, Acc: 0.841983\n",
            "Validation Loss: 1.163415, Acc: 0.645000\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:95.841684, Acc:0.504505.\n",
            "\n",
            "Class:2, Testing Loss:83.917938, Acc:0.535433.\n",
            "\n",
            "Class:3, Testing Loss:108.717945, Acc:0.635417.\n",
            "\n",
            "Class:4, Testing Loss:51.513849, Acc:0.743119.\n",
            "\n",
            "Class:0, Testing Loss:115.870086, Acc:0.600451.\n",
            "\n",
            "#############################\n",
            "epoch 61\n",
            "**********\n",
            "Finish 61 epoch, Loss: 0.986404, Acc: 0.847230\n",
            "Validation Loss: 1.086007, Acc: 0.635000\n",
            "\n",
            "epoch 62\n",
            "**********\n",
            "Finish 62 epoch, Loss: 0.970120, Acc: 0.865889\n",
            "Validation Loss: 1.107437, Acc: 0.650000\n",
            "\n",
            "epoch 63\n",
            "**********\n",
            "Finish 63 epoch, Loss: 0.955291, Acc: 0.862974\n",
            "Validation Loss: 1.081445, Acc: 0.642500\n",
            "\n",
            "epoch 64\n",
            "**********\n",
            "Finish 64 epoch, Loss: 0.851844, Acc: 0.869388\n",
            "Validation Loss: 1.221515, Acc: 0.632500\n",
            "\n",
            "epoch 65\n",
            "**********\n",
            "Finish 65 epoch, Loss: 0.889640, Acc: 0.863557\n",
            "Validation Loss: 1.122707, Acc: 0.657500\n",
            "\n",
            "epoch 66\n",
            "**********\n",
            "Finish 66 epoch, Loss: 0.906979, Acc: 0.869388\n",
            "Validation Loss: 1.118151, Acc: 0.650000\n",
            "\n",
            "epoch 67\n",
            "**********\n",
            "Finish 67 epoch, Loss: 0.856711, Acc: 0.875219\n",
            "Validation Loss: 1.202753, Acc: 0.615000\n",
            "\n",
            "epoch 68\n",
            "**********\n",
            "Finish 68 epoch, Loss: 0.862317, Acc: 0.864723\n",
            "Validation Loss: 1.217454, Acc: 0.620000\n",
            "\n",
            "epoch 69\n",
            "**********\n",
            "Finish 69 epoch, Loss: 0.874065, Acc: 0.874636\n",
            "Validation Loss: 1.301238, Acc: 0.637500\n",
            "\n",
            "epoch 70\n",
            "**********\n",
            "Finish 70 epoch, Loss: 0.910763, Acc: 0.875802\n",
            "Validation Loss: 1.177347, Acc: 0.642500\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:79.094268, Acc:0.603604.\n",
            "\n",
            "Class:2, Testing Loss:68.447628, Acc:0.637795.\n",
            "\n",
            "Class:3, Testing Loss:153.261795, Acc:0.572917.\n",
            "\n",
            "Class:4, Testing Loss:57.364872, Acc:0.688073.\n",
            "\n",
            "Class:0, Testing Loss:111.958772, Acc:0.627540.\n",
            "\n",
            "#############################\n",
            "epoch 71\n",
            "**********\n",
            "Finish 71 epoch, Loss: 0.765367, Acc: 0.888630\n",
            "Validation Loss: 1.224306, Acc: 0.645000\n",
            "\n",
            "epoch 72\n",
            "**********\n",
            "Finish 72 epoch, Loss: 0.777771, Acc: 0.880466\n",
            "Validation Loss: 1.278575, Acc: 0.637500\n",
            "\n",
            "epoch 73\n",
            "**********\n",
            "Finish 73 epoch, Loss: 0.823212, Acc: 0.880466\n",
            "Validation Loss: 1.115171, Acc: 0.627500\n",
            "\n",
            "epoch 74\n",
            "**********\n",
            "Finish 74 epoch, Loss: 0.819887, Acc: 0.885131\n",
            "Validation Loss: 1.248873, Acc: 0.612500\n",
            "\n",
            "epoch 75\n",
            "**********\n",
            "Finish 75 epoch, Loss: 0.765006, Acc: 0.886880\n",
            "Validation Loss: 1.199456, Acc: 0.627500\n",
            "\n",
            "epoch 76\n",
            "**********\n",
            "Finish 76 epoch, Loss: 0.794216, Acc: 0.884548\n",
            "Validation Loss: 1.179234, Acc: 0.647500\n",
            "\n",
            "epoch 77\n",
            "**********\n",
            "Finish 77 epoch, Loss: 0.955296, Acc: 0.868805\n",
            "Validation Loss: 1.270887, Acc: 0.640000\n",
            "\n",
            "epoch 78\n",
            "**********\n",
            "Finish 78 epoch, Loss: 0.859898, Acc: 0.883965\n",
            "Validation Loss: 1.207481, Acc: 0.645000\n",
            "\n",
            "epoch 79\n",
            "**********\n",
            "Finish 79 epoch, Loss: 0.732164, Acc: 0.891545\n",
            "Validation Loss: 1.197050, Acc: 0.640000\n",
            "\n",
            "epoch 80\n",
            "**********\n",
            "Finish 80 epoch, Loss: 0.695334, Acc: 0.902624\n",
            "Validation Loss: 1.251539, Acc: 0.617500\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:84.547990, Acc:0.576577.\n",
            "\n",
            "Class:2, Testing Loss:66.829159, Acc:0.661417.\n",
            "\n",
            "Class:3, Testing Loss:177.353497, Acc:0.510417.\n",
            "\n",
            "Class:4, Testing Loss:55.885674, Acc:0.715596.\n",
            "\n",
            "Class:0, Testing Loss:117.405554, Acc:0.620767.\n",
            "\n",
            "#############################\n",
            "epoch 81\n",
            "**********\n",
            "Finish 81 epoch, Loss: 0.735059, Acc: 0.886297\n",
            "Validation Loss: 1.202801, Acc: 0.640000\n",
            "\n",
            "epoch 82\n",
            "**********\n",
            "Finish 82 epoch, Loss: 0.833205, Acc: 0.889213\n",
            "Validation Loss: 1.210074, Acc: 0.635000\n",
            "\n",
            "epoch 83\n",
            "**********\n",
            "Finish 83 epoch, Loss: 0.683103, Acc: 0.909038\n",
            "Validation Loss: 1.302623, Acc: 0.647500\n",
            "\n",
            "epoch 84\n",
            "**********\n",
            "Finish 84 epoch, Loss: 0.715239, Acc: 0.898542\n",
            "Validation Loss: 1.244523, Acc: 0.627500\n",
            "\n",
            "epoch 85\n",
            "**********\n",
            "Finish 85 epoch, Loss: 0.722502, Acc: 0.888047\n",
            "Validation Loss: 1.094869, Acc: 0.665000\n",
            "\n",
            "epoch 86\n",
            "**********\n",
            "Finish 86 epoch, Loss: 0.675603, Acc: 0.896210\n",
            "Validation Loss: 1.271472, Acc: 0.610000\n",
            "\n",
            "epoch 87\n",
            "**********\n",
            "Finish 87 epoch, Loss: 0.685333, Acc: 0.910204\n",
            "Validation Loss: 1.322107, Acc: 0.635000\n",
            "\n",
            "epoch 88\n",
            "**********\n",
            "Finish 88 epoch, Loss: 0.680340, Acc: 0.896793\n",
            "Validation Loss: 1.297666, Acc: 0.655000\n",
            "\n",
            "epoch 89\n",
            "**********\n",
            "Finish 89 epoch, Loss: 0.690090, Acc: 0.900292\n",
            "Validation Loss: 1.249788, Acc: 0.635000\n",
            "\n",
            "epoch 90\n",
            "**********\n",
            "Finish 90 epoch, Loss: 0.742111, Acc: 0.888047\n",
            "Validation Loss: 1.286033, Acc: 0.630000\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:75.343525, Acc:0.648649.\n",
            "\n",
            "Class:2, Testing Loss:72.693447, Acc:0.653543.\n",
            "\n",
            "Class:3, Testing Loss:192.583305, Acc:0.447917.\n",
            "\n",
            "Class:4, Testing Loss:58.997442, Acc:0.715596.\n",
            "\n",
            "Class:0, Testing Loss:121.086004, Acc:0.623025.\n",
            "\n",
            "#############################\n",
            "epoch 91\n",
            "**********\n",
            "Finish 91 epoch, Loss: 0.636661, Acc: 0.913120\n",
            "Validation Loss: 1.280166, Acc: 0.637500\n",
            "\n",
            "epoch 92\n",
            "**********\n",
            "Finish 92 epoch, Loss: 0.667202, Acc: 0.913120\n",
            "Validation Loss: 1.246830, Acc: 0.645000\n",
            "\n",
            "epoch 93\n",
            "**********\n",
            "Finish 93 epoch, Loss: 0.550017, Acc: 0.916618\n",
            "Validation Loss: 1.264613, Acc: 0.637500\n",
            "\n",
            "epoch 94\n",
            "**********\n",
            "Finish 94 epoch, Loss: 0.559655, Acc: 0.930612\n",
            "Validation Loss: 1.247731, Acc: 0.652500\n",
            "\n",
            "epoch 95\n",
            "**********\n",
            "Finish 95 epoch, Loss: 0.480394, Acc: 0.930029\n",
            "Validation Loss: 1.283662, Acc: 0.635000\n",
            "\n",
            "epoch 96\n",
            "**********\n",
            "Finish 96 epoch, Loss: 0.623305, Acc: 0.906122\n",
            "Validation Loss: 1.315313, Acc: 0.645000\n",
            "\n",
            "epoch 97\n",
            "**********\n",
            "Finish 97 epoch, Loss: 0.521588, Acc: 0.920700\n",
            "Validation Loss: 1.314335, Acc: 0.637500\n",
            "\n",
            "epoch 98\n",
            "**********\n",
            "Finish 98 epoch, Loss: 0.679846, Acc: 0.900875\n",
            "Validation Loss: 1.351616, Acc: 0.620000\n",
            "\n",
            "epoch 99\n",
            "**********\n",
            "Finish 99 epoch, Loss: 0.653595, Acc: 0.907289\n",
            "Validation Loss: 1.405637, Acc: 0.617500\n",
            "\n",
            "epoch 100\n",
            "**********\n",
            "Finish 100 epoch, Loss: 0.661626, Acc: 0.914286\n",
            "Validation Loss: 1.271023, Acc: 0.652500\n",
            "\n",
            "#############################\n",
            "Class:1, Testing Loss:82.299007, Acc:0.612613.\n",
            "\n",
            "Class:2, Testing Loss:82.428249, Acc:0.598425.\n",
            "\n",
            "Class:3, Testing Loss:109.991615, Acc:0.677083.\n",
            "\n",
            "Class:4, Testing Loss:69.279074, Acc:0.688073.\n",
            "\n",
            "Class:0, Testing Loss:115.446417, Acc:0.641084.\n",
            "\n",
            "#############################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gz4UzU2SdPXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "record_train = train_acc\n",
        "record_val = val_acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "menXIzPqdU3g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open('record.txt','w')\n",
        "train_acc_str = \"\"\n",
        "val_acc_str = \"\"\n",
        "for e in record_train:\n",
        "  train_acc_str += str(e)\n",
        "  train_acc_str += ','\n",
        "for e in record_val:\n",
        "  val_acc_str += str(e)\n",
        "  val_acc_str += ','\n",
        "  \n",
        "file.writelines(train_acc_str)\n",
        "file.writelines(val_acc_str)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MQR6CBJzeANc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4879f8b6-b699-43a5-9f51-5451e2ce7abb"
      },
      "cell_type": "code",
      "source": [
        "file = open('record.txt','r')\n",
        "s = file.read()\n",
        "print(s)\n",
        "file.close()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2816326530612245,0.3457725947521866,0.40233236151603496,0.46705539358600584,0.4956268221574344,0.5148688046647231,0.5399416909620991,0.5446064139941691,0.5446064139941691,0.5836734693877551,0.5743440233236151,0.6081632653061224,0.6104956268221574,0.6256559766763848,0.6192419825072887,0.6379008746355685,0.6658892128279883,0.6699708454810496,0.6600583090379009,0.6559766763848397,0.6746355685131196,0.6565597667638484,0.682798833819242,0.6868804664723032,0.7107871720116619,0.7061224489795919,0.7061224489795919,0.7294460641399417,0.721865889212828,0.7236151603498542,0.7393586005830903,0.7510204081632653,0.753935860058309,0.7527696793002916,0.7568513119533528,0.7568513119533528,0.758600583090379,0.7679300291545189,0.7685131195335277,0.7702623906705539,0.7725947521865889,0.7807580174927113,0.7848396501457726,0.8023323615160349,0.8011661807580175,0.8029154518950438,0.7965014577259475,0.8174927113702624,0.8198250728862974,0.8163265306122449,0.8239067055393586,0.8408163265306122,0.8332361516034985,0.8583090379008746,0.8384839650145772,0.8303206997084548,0.8326530612244898,0.8594752186588921,0.8618075801749271,0.8419825072886298,0.8472303206997085,0.8658892128279884,0.8629737609329446,0.8693877551020408,0.8635568513119534,0.8693877551020408,0.8752186588921282,0.8647230320699708,0.8746355685131195,0.8758017492711371,0.8886297376093294,0.880466472303207,0.880466472303207,0.885131195335277,0.8868804664723032,0.8845481049562682,0.8688046647230321,0.8839650145772595,0.8915451895043732,0.9026239067055394,0.8862973760932945,0.8892128279883382,0.9090379008746355,0.8985422740524781,0.8880466472303207,0.8962099125364431,0.9102040816326531,0.8967930029154519,0.9002915451895044,0.8880466472303207,0.9131195335276968,0.9131195335276968,0.9166180758017493,0.9306122448979591,0.9300291545189504,0.9061224489795918,0.9206997084548105,0.9008746355685131,0.9072886297376094,0.9142857142857143,0.34,0.4775,0.4975,0.365,0.545,0.555,0.55,0.56,0.5775,0.5,0.5625,0.59,0.55,0.605,0.545,0.5975,0.585,0.5975,0.605,0.5875,0.5825,0.585,0.5925,0.605,0.62,0.59,0.585,0.6025,0.6225,0.6025,0.615,0.635,0.6175,0.6075,0.5875,0.61,0.6175,0.6075,0.615,0.6175,0.635,0.6,0.625,0.6275,0.6075,0.635,0.62,0.635,0.6175,0.645,0.6275,0.625,0.63,0.625,0.65,0.6025,0.62,0.645,0.635,0.645,0.635,0.65,0.6425,0.6325,0.6575,0.65,0.615,0.62,0.6375,0.6425,0.645,0.6375,0.6275,0.6125,0.6275,0.6475,0.64,0.645,0.64,0.6175,0.64,0.635,0.6475,0.6275,0.665,0.61,0.635,0.655,0.635,0.63,0.6375,0.645,0.6375,0.6525,0.635,0.645,0.6375,0.62,0.6175,0.6525,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3RY0NzXQeL9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cc64ca86-babc-4d55-b57f-f078b4bd3ff7"
      },
      "cell_type": "code",
      "source": [
        "record = s.split(',')\n",
        "print(record)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0.2816326530612245', '0.3457725947521866', '0.40233236151603496', '0.46705539358600584', '0.4956268221574344', '0.5148688046647231', '0.5399416909620991', '0.5446064139941691', '0.5446064139941691', '0.5836734693877551', '0.5743440233236151', '0.6081632653061224', '0.6104956268221574', '0.6256559766763848', '0.6192419825072887', '0.6379008746355685', '0.6658892128279883', '0.6699708454810496', '0.6600583090379009', '0.6559766763848397', '0.6746355685131196', '0.6565597667638484', '0.682798833819242', '0.6868804664723032', '0.7107871720116619', '0.7061224489795919', '0.7061224489795919', '0.7294460641399417', '0.721865889212828', '0.7236151603498542', '0.7393586005830903', '0.7510204081632653', '0.753935860058309', '0.7527696793002916', '0.7568513119533528', '0.7568513119533528', '0.758600583090379', '0.7679300291545189', '0.7685131195335277', '0.7702623906705539', '0.7725947521865889', '0.7807580174927113', '0.7848396501457726', '0.8023323615160349', '0.8011661807580175', '0.8029154518950438', '0.7965014577259475', '0.8174927113702624', '0.8198250728862974', '0.8163265306122449', '0.8239067055393586', '0.8408163265306122', '0.8332361516034985', '0.8583090379008746', '0.8384839650145772', '0.8303206997084548', '0.8326530612244898', '0.8594752186588921', '0.8618075801749271', '0.8419825072886298', '0.8472303206997085', '0.8658892128279884', '0.8629737609329446', '0.8693877551020408', '0.8635568513119534', '0.8693877551020408', '0.8752186588921282', '0.8647230320699708', '0.8746355685131195', '0.8758017492711371', '0.8886297376093294', '0.880466472303207', '0.880466472303207', '0.885131195335277', '0.8868804664723032', '0.8845481049562682', '0.8688046647230321', '0.8839650145772595', '0.8915451895043732', '0.9026239067055394', '0.8862973760932945', '0.8892128279883382', '0.9090379008746355', '0.8985422740524781', '0.8880466472303207', '0.8962099125364431', '0.9102040816326531', '0.8967930029154519', '0.9002915451895044', '0.8880466472303207', '0.9131195335276968', '0.9131195335276968', '0.9166180758017493', '0.9306122448979591', '0.9300291545189504', '0.9061224489795918', '0.9206997084548105', '0.9008746355685131', '0.9072886297376094', '0.9142857142857143', '0.34', '0.4775', '0.4975', '0.365', '0.545', '0.555', '0.55', '0.56', '0.5775', '0.5', '0.5625', '0.59', '0.55', '0.605', '0.545', '0.5975', '0.585', '0.5975', '0.605', '0.5875', '0.5825', '0.585', '0.5925', '0.605', '0.62', '0.59', '0.585', '0.6025', '0.6225', '0.6025', '0.615', '0.635', '0.6175', '0.6075', '0.5875', '0.61', '0.6175', '0.6075', '0.615', '0.6175', '0.635', '0.6', '0.625', '0.6275', '0.6075', '0.635', '0.62', '0.635', '0.6175', '0.645', '0.6275', '0.625', '0.63', '0.625', '0.65', '0.6025', '0.62', '0.645', '0.635', '0.645', '0.635', '0.65', '0.6425', '0.6325', '0.6575', '0.65', '0.615', '0.62', '0.6375', '0.6425', '0.645', '0.6375', '0.6275', '0.6125', '0.6275', '0.6475', '0.64', '0.645', '0.64', '0.6175', '0.64', '0.635', '0.6475', '0.6275', '0.665', '0.61', '0.635', '0.655', '0.635', '0.63', '0.6375', '0.645', '0.6375', '0.6525', '0.635', '0.645', '0.6375', '0.62', '0.6175', '0.6525', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8dMG1ItmejDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "94a884b3-a32e-4a07-ddb8-16b34df829c3"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(train_acc, label='train')\n",
        "plt.plot(val_acc, label='val')\n",
        "\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.title(\"The accuracy against epoch\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GyMurZKam7_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_people(subject):\n",
        "  model.eval()\n",
        "  eval_loss = 0.\n",
        "  eval_acc = 0.\n",
        "  total = 0.\n",
        "  \n",
        "  for batch_idx, (signals, tags) in enumerate(testloader):\n",
        "    signals = signals.squeeze(1)\n",
        "    signals = signals.permute(2, 0, 1)\n",
        "    if use_gpu:\n",
        "        signals = Variable(signals).cuda()\n",
        "        tags = Variable(tags).cuda()\n",
        "    else:\n",
        "        signals = Variable(signals)\n",
        "        tags = Variable(tags)\n",
        "\n",
        "    tag_scores = model(signals)\n",
        "#     for i in range(9):\n",
        "#       tmp_index = tags.index(i)\n",
        "#       tmo_tag = tag\n",
        "    loss = criterion(tag_scores, tags)\n",
        "    #print(tags)\n",
        "    eval_loss += loss.data.item() * tags.size(0)\n",
        "    _, pred = torch.max(tag_scores, 1)\n",
        "    total += tags.size(0)\n",
        "    num_correct = (pred == tags).sum()\n",
        "    eval_acc += num_correct.data.item()\n",
        "  print('people:{}, Testing Loss:{:.6f}, Acc:{:6f}.'.format(subject+1, eval_loss/len(testloader), eval_acc / total))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ukR_PyBeq32j",
        "colab_type": "code",
        "outputId": "b82116ce-0627-4648-ba5e-95dc6acde8f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "x_test_list=[[] for i in range(9)]\n",
        "y_test_list=[[] for i in range(9)]\n",
        "for i in range(len(person_test)):\n",
        "  x_test_list[int(person_test[i][0])].append(X_test[i, :22, :])\n",
        "  y_test_list[int(person_test[i][0])].append(y_test[i])\n",
        "\n",
        "x_test_08=np.array(x_test_list)\n",
        "y_test_08=np.array(y_test_list)\n",
        "\n",
        "\n",
        "print(y_test_list)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[770, 769, 771, 772, 772, 771, 771, 772, 772, 769, 771, 772, 771, 769, 770, 770, 772, 772, 771, 769, 771, 770, 769, 770, 772, 771, 769, 770, 770, 772, 771, 771, 770, 770, 771, 769, 769, 769, 769, 772, 770, 771, 769, 770, 772, 771, 769, 772, 771, 770], [770, 771, 769, 772, 772, 770, 770, 772, 772, 771, 770, 770, 772, 770, 769, 770, 770, 770, 771, 769, 769, 770, 772, 771, 771, 769, 770, 771, 769, 771, 772, 771, 769, 770, 772, 769, 769, 772, 772, 769, 772, 771, 770, 770, 769, 771, 769, 772, 770, 772], [770, 769, 769, 770, 771, 769, 769, 772, 771, 771, 772, 770, 772, 770, 769, 769, 770, 772, 770, 769, 769, 769, 772, 771, 772, 769, 772, 769, 771, 772, 771, 770, 772, 770, 770, 772, 769, 770, 772, 769, 769, 771, 771, 769, 771, 772, 772, 770, 769, 772], [769, 772, 770, 771, 770, 769, 771, 769, 770, 772, 769, 770, 771, 771, 770, 771, 772, 770, 769, 772, 770, 770, 770, 772, 771, 769, 770, 771, 770, 772, 771, 772, 772, 770, 770, 772, 770, 771, 771, 770, 772, 771, 772, 772, 771, 771, 770, 772, 771, 770], [772, 770, 769, 769, 772, 771, 769, 769, 769, 772, 772, 771, 769, 770, 769, 771, 769, 769, 772, 771, 770, 770, 772, 770, 769, 771, 769, 770, 771, 772, 770, 770, 771, 769, 769, 770, 769, 772, 772, 772, 771, 769, 769, 770, 770, 772, 769], [772, 769, 771, 772, 769, 770, 769, 769, 769, 772, 770, 771, 769, 770, 772, 769, 769, 771, 772, 772, 771, 770, 769, 769, 772, 771, 771, 772, 770, 770, 772, 771, 769, 770, 769, 769, 771, 770, 771, 769, 771, 770, 770, 769, 770, 770, 770, 770, 770], [770, 769, 771, 770, 769, 769, 770, 769, 772, 771, 772, 772, 772, 770, 771, 769, 771, 771, 772, 770, 771, 769, 769, 769, 770, 769, 770, 772, 769, 770, 770, 770, 771, 772, 770, 770, 772, 769, 769, 770, 770, 770, 771, 770, 770, 772, 769, 770, 772, 772], [772, 772, 770, 771, 772, 770, 771, 770, 769, 772, 771, 771, 772, 771, 769, 770, 772, 769, 772, 770, 770, 772, 772, 770, 770, 772, 769, 772, 770, 770, 772, 771, 769, 769, 772, 770, 771, 772, 769, 770, 770, 772, 771, 770, 772, 771, 770, 770, 771, 771], [769, 771, 769, 772, 770, 771, 771, 769, 771, 769, 771, 769, 772, 772, 772, 771, 772, 771, 769, 770, 769, 771, 770, 771, 772, 772, 770, 770, 772, 769, 770, 770, 771, 769, 771, 770, 772, 770, 769, 770, 769, 769, 770, 769, 772, 771, 770]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5NFm8IvZp-Mq",
        "colab_type": "code",
        "outputId": "30e63e4b-533b-4bb4-e0a5-640a80c2137e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12257
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(num_epoches):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "  train(epoch)\n",
        "\n",
        "  if epoch%10==9:\n",
        "    print(\"#############################\")\n",
        "    for i in range(9):\n",
        "      testset = torch.utils.data.TensorDataset(torch.from_numpy(np.array(x_test_08[i])).float(), torch.from_numpy(np.array(y_test_08[i]) - 769).long())\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "      test_people(i)\n",
        "    testset = torch.utils.data.TensorDataset(torch.from_numpy(X_test[:, :22, :]).float(), torch.from_numpy(y_test - 769).long())\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_people(-1)\n",
        "    print(\"#############################\")\n",
        "\n",
        "  if train_acc[-1]>95:\n",
        "    break\n",
        "\n",
        "        "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "**********\n",
            "Finish 1 epoch, Loss: 3.376347, Acc: 0.282799\n",
            "Validation Loss: 1.327142, Acc: 0.310000\n",
            "\n",
            "epoch 2\n",
            "**********\n",
            "Finish 2 epoch, Loss: 3.205587, Acc: 0.357434\n",
            "Validation Loss: 1.289492, Acc: 0.382500\n",
            "\n",
            "epoch 3\n",
            "**********\n",
            "Finish 3 epoch, Loss: 3.036238, Acc: 0.402915\n",
            "Validation Loss: 1.191168, Acc: 0.447500\n",
            "\n",
            "epoch 4\n",
            "**********\n",
            "Finish 4 epoch, Loss: 2.961383, Acc: 0.451895\n",
            "Validation Loss: 1.185003, Acc: 0.427500\n",
            "\n",
            "epoch 5\n",
            "**********\n",
            "Finish 5 epoch, Loss: 2.846095, Acc: 0.488047\n",
            "Validation Loss: 1.140660, Acc: 0.470000\n",
            "\n",
            "epoch 6\n",
            "**********\n",
            "Finish 6 epoch, Loss: 2.756713, Acc: 0.505539\n",
            "Validation Loss: 1.091038, Acc: 0.477500\n",
            "\n",
            "epoch 7\n",
            "**********\n",
            "Finish 7 epoch, Loss: 2.580185, Acc: 0.520700\n",
            "Validation Loss: 1.064045, Acc: 0.530000\n",
            "\n",
            "epoch 8\n",
            "**********\n",
            "Finish 8 epoch, Loss: 2.586823, Acc: 0.541108\n",
            "Validation Loss: 1.088995, Acc: 0.545000\n",
            "\n",
            "epoch 9\n",
            "**********\n",
            "Finish 9 epoch, Loss: 2.465375, Acc: 0.566181\n",
            "Validation Loss: 1.084972, Acc: 0.507500\n",
            "\n",
            "epoch 10\n",
            "**********\n",
            "Finish 10 epoch, Loss: 2.362438, Acc: 0.603499\n",
            "Validation Loss: 1.036798, Acc: 0.545000\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:68.612325, Acc:0.440000.\n",
            "\n",
            "people:2, Testing Loss:61.861724, Acc:0.420000.\n",
            "\n",
            "people:3, Testing Loss:55.027443, Acc:0.460000.\n",
            "\n",
            "people:4, Testing Loss:47.154588, Acc:0.680000.\n",
            "\n",
            "people:5, Testing Loss:42.713617, Acc:0.659574.\n",
            "\n",
            "people:6, Testing Loss:61.679963, Acc:0.489796.\n",
            "\n",
            "people:7, Testing Loss:40.164858, Acc:0.680000.\n",
            "\n",
            "people:8, Testing Loss:54.862529, Acc:0.560000.\n",
            "\n",
            "people:9, Testing Loss:49.694477, Acc:0.489362.\n",
            "\n",
            "people:0, Testing Loss:96.293243, Acc:0.541761.\n",
            "\n",
            "#############################\n",
            "epoch 11\n",
            "**********\n",
            "Finish 11 epoch, Loss: 2.345738, Acc: 0.606414\n",
            "Validation Loss: 1.062456, Acc: 0.560000\n",
            "\n",
            "epoch 12\n",
            "**********\n",
            "Finish 12 epoch, Loss: 2.318024, Acc: 0.620991\n",
            "Validation Loss: 1.058956, Acc: 0.537500\n",
            "\n",
            "epoch 13\n",
            "**********\n",
            "Finish 13 epoch, Loss: 2.202388, Acc: 0.619825\n",
            "Validation Loss: 1.049443, Acc: 0.555000\n",
            "\n",
            "epoch 14\n",
            "**********\n",
            "Finish 14 epoch, Loss: 2.130677, Acc: 0.647230\n",
            "Validation Loss: 1.080102, Acc: 0.520000\n",
            "\n",
            "epoch 15\n",
            "**********\n",
            "Finish 15 epoch, Loss: 2.152436, Acc: 0.641399\n",
            "Validation Loss: 1.033283, Acc: 0.550000\n",
            "\n",
            "epoch 16\n",
            "**********\n",
            "Finish 16 epoch, Loss: 2.116368, Acc: 0.654227\n",
            "Validation Loss: 1.048903, Acc: 0.552500\n",
            "\n",
            "epoch 17\n",
            "**********\n",
            "Finish 17 epoch, Loss: 1.986215, Acc: 0.677551\n",
            "Validation Loss: 1.043834, Acc: 0.555000\n",
            "\n",
            "epoch 18\n",
            "**********\n",
            "Finish 18 epoch, Loss: 2.051222, Acc: 0.649563\n",
            "Validation Loss: 1.094127, Acc: 0.562500\n",
            "\n",
            "epoch 19\n",
            "**********\n",
            "Finish 19 epoch, Loss: 2.045556, Acc: 0.675219\n",
            "Validation Loss: 1.005082, Acc: 0.567500\n",
            "\n",
            "epoch 20\n",
            "**********\n",
            "Finish 20 epoch, Loss: 1.994534, Acc: 0.697959\n",
            "Validation Loss: 1.063034, Acc: 0.570000\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:69.597739, Acc:0.480000.\n",
            "\n",
            "people:2, Testing Loss:66.518325, Acc:0.420000.\n",
            "\n",
            "people:3, Testing Loss:47.642249, Acc:0.620000.\n",
            "\n",
            "people:4, Testing Loss:48.677909, Acc:0.600000.\n",
            "\n",
            "people:5, Testing Loss:38.563764, Acc:0.744681.\n",
            "\n",
            "people:6, Testing Loss:59.311626, Acc:0.571429.\n",
            "\n",
            "people:7, Testing Loss:43.590221, Acc:0.600000.\n",
            "\n",
            "people:8, Testing Loss:62.774408, Acc:0.520000.\n",
            "\n",
            "people:9, Testing Loss:34.878966, Acc:0.723404.\n",
            "\n",
            "people:0, Testing Loss:94.370177, Acc:0.584650.\n",
            "\n",
            "#############################\n",
            "epoch 21\n",
            "**********\n",
            "Finish 21 epoch, Loss: 1.960823, Acc: 0.691545\n",
            "Validation Loss: 1.039485, Acc: 0.600000\n",
            "\n",
            "epoch 22\n",
            "**********\n",
            "Finish 22 epoch, Loss: 1.835422, Acc: 0.702041\n",
            "Validation Loss: 1.083392, Acc: 0.555000\n",
            "\n",
            "epoch 23\n",
            "**********\n",
            "Finish 23 epoch, Loss: 1.812447, Acc: 0.715452\n",
            "Validation Loss: 1.120547, Acc: 0.545000\n",
            "\n",
            "epoch 24\n",
            "**********\n",
            "Finish 24 epoch, Loss: 1.899397, Acc: 0.690962\n",
            "Validation Loss: 1.082090, Acc: 0.540000\n",
            "\n",
            "epoch 25\n",
            "**********\n",
            "Finish 25 epoch, Loss: 1.862276, Acc: 0.696210\n",
            "Validation Loss: 1.031334, Acc: 0.562500\n",
            "\n",
            "epoch 26\n",
            "**********\n",
            "Finish 26 epoch, Loss: 1.860902, Acc: 0.710204\n",
            "Validation Loss: 1.036489, Acc: 0.572500\n",
            "\n",
            "epoch 27\n",
            "**********\n",
            "Finish 27 epoch, Loss: 1.670018, Acc: 0.717784\n",
            "Validation Loss: 1.030679, Acc: 0.582500\n",
            "\n",
            "epoch 28\n",
            "**********\n",
            "Finish 28 epoch, Loss: 1.728846, Acc: 0.728863\n",
            "Validation Loss: 1.014199, Acc: 0.580000\n",
            "\n",
            "epoch 29\n",
            "**********\n",
            "Finish 29 epoch, Loss: 1.643200, Acc: 0.739942\n",
            "Validation Loss: 1.040071, Acc: 0.580000\n",
            "\n",
            "epoch 30\n",
            "**********\n",
            "Finish 30 epoch, Loss: 1.619898, Acc: 0.753936\n",
            "Validation Loss: 1.070740, Acc: 0.535000\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:70.151979, Acc:0.520000.\n",
            "\n",
            "people:2, Testing Loss:60.333437, Acc:0.440000.\n",
            "\n",
            "people:3, Testing Loss:55.304712, Acc:0.520000.\n",
            "\n",
            "people:4, Testing Loss:47.016466, Acc:0.620000.\n",
            "\n",
            "people:5, Testing Loss:36.951189, Acc:0.659574.\n",
            "\n",
            "people:6, Testing Loss:46.951373, Acc:0.653061.\n",
            "\n",
            "people:7, Testing Loss:40.951017, Acc:0.620000.\n",
            "\n",
            "people:8, Testing Loss:63.830328, Acc:0.540000.\n",
            "\n",
            "people:9, Testing Loss:44.236695, Acc:0.638298.\n",
            "\n",
            "people:0, Testing Loss:93.068999, Acc:0.577878.\n",
            "\n",
            "#############################\n",
            "epoch 31\n",
            "**********\n",
            "Finish 31 epoch, Loss: 1.602478, Acc: 0.755685\n",
            "Validation Loss: 1.105847, Acc: 0.577500\n",
            "\n",
            "epoch 32\n",
            "**********\n",
            "Finish 32 epoch, Loss: 1.537125, Acc: 0.741108\n",
            "Validation Loss: 1.080471, Acc: 0.575000\n",
            "\n",
            "epoch 33\n",
            "**********\n",
            "Finish 33 epoch, Loss: 1.647975, Acc: 0.735860\n",
            "Validation Loss: 1.030877, Acc: 0.597500\n",
            "\n",
            "epoch 34\n",
            "**********\n",
            "Finish 34 epoch, Loss: 1.622681, Acc: 0.749271\n",
            "Validation Loss: 1.034006, Acc: 0.587500\n",
            "\n",
            "epoch 35\n",
            "**********\n",
            "Finish 35 epoch, Loss: 1.457219, Acc: 0.776093\n",
            "Validation Loss: 1.127615, Acc: 0.557500\n",
            "\n",
            "epoch 36\n",
            "**********\n",
            "Finish 36 epoch, Loss: 1.430118, Acc: 0.779009\n",
            "Validation Loss: 1.104453, Acc: 0.572500\n",
            "\n",
            "epoch 37\n",
            "**********\n",
            "Finish 37 epoch, Loss: 1.382221, Acc: 0.774344\n",
            "Validation Loss: 1.076801, Acc: 0.597500\n",
            "\n",
            "epoch 38\n",
            "**********\n",
            "Finish 38 epoch, Loss: 1.359278, Acc: 0.793586\n",
            "Validation Loss: 1.132096, Acc: 0.587500\n",
            "\n",
            "epoch 39\n",
            "**********\n",
            "Finish 39 epoch, Loss: 1.404768, Acc: 0.802332\n",
            "Validation Loss: 1.195728, Acc: 0.570000\n",
            "\n",
            "epoch 40\n",
            "**********\n",
            "Finish 40 epoch, Loss: 1.397968, Acc: 0.783673\n",
            "Validation Loss: 1.197587, Acc: 0.590000\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:78.183937, Acc:0.460000.\n",
            "\n",
            "people:2, Testing Loss:66.986686, Acc:0.500000.\n",
            "\n",
            "people:3, Testing Loss:57.883102, Acc:0.640000.\n",
            "\n",
            "people:4, Testing Loss:45.029747, Acc:0.680000.\n",
            "\n",
            "people:5, Testing Loss:35.930333, Acc:0.659574.\n",
            "\n",
            "people:6, Testing Loss:61.876159, Acc:0.612245.\n",
            "\n",
            "people:7, Testing Loss:46.637955, Acc:0.680000.\n",
            "\n",
            "people:8, Testing Loss:75.858659, Acc:0.600000.\n",
            "\n",
            "people:9, Testing Loss:43.671464, Acc:0.744681.\n",
            "\n",
            "people:0, Testing Loss:102.375713, Acc:0.618510.\n",
            "\n",
            "#############################\n",
            "epoch 41\n",
            "**********\n",
            "Finish 41 epoch, Loss: 1.259822, Acc: 0.806414\n",
            "Validation Loss: 1.183451, Acc: 0.577500\n",
            "\n",
            "epoch 42\n",
            "**********\n",
            "Finish 42 epoch, Loss: 1.324482, Acc: 0.808746\n",
            "Validation Loss: 1.194864, Acc: 0.582500\n",
            "\n",
            "epoch 43\n",
            "**********\n",
            "Finish 43 epoch, Loss: 1.263371, Acc: 0.800000\n",
            "Validation Loss: 1.164640, Acc: 0.580000\n",
            "\n",
            "epoch 44\n",
            "**********\n",
            "Finish 44 epoch, Loss: 1.172428, Acc: 0.813994\n",
            "Validation Loss: 1.125882, Acc: 0.592500\n",
            "\n",
            "epoch 45\n",
            "**********\n",
            "Finish 45 epoch, Loss: 1.263351, Acc: 0.809329\n",
            "Validation Loss: 1.098874, Acc: 0.577500\n",
            "\n",
            "epoch 46\n",
            "**********\n",
            "Finish 46 epoch, Loss: 1.146874, Acc: 0.820991\n",
            "Validation Loss: 1.270364, Acc: 0.575000\n",
            "\n",
            "epoch 47\n",
            "**********\n",
            "Finish 47 epoch, Loss: 1.254635, Acc: 0.800583\n",
            "Validation Loss: 1.257252, Acc: 0.577500\n",
            "\n",
            "epoch 48\n",
            "**********\n",
            "Finish 48 epoch, Loss: 1.148684, Acc: 0.813411\n",
            "Validation Loss: 1.120123, Acc: 0.575000\n",
            "\n",
            "epoch 49\n",
            "**********\n",
            "Finish 49 epoch, Loss: 1.248945, Acc: 0.815160\n",
            "Validation Loss: 1.156137, Acc: 0.587500\n",
            "\n",
            "epoch 50\n",
            "**********\n",
            "Finish 50 epoch, Loss: 1.107992, Acc: 0.839650\n",
            "Validation Loss: 1.199207, Acc: 0.577500\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:78.968817, Acc:0.500000.\n",
            "\n",
            "people:2, Testing Loss:58.158565, Acc:0.480000.\n",
            "\n",
            "people:3, Testing Loss:58.668000, Acc:0.560000.\n",
            "\n",
            "people:4, Testing Loss:47.810096, Acc:0.640000.\n",
            "\n",
            "people:5, Testing Loss:38.240934, Acc:0.659574.\n",
            "\n",
            "people:6, Testing Loss:63.067986, Acc:0.612245.\n",
            "\n",
            "people:7, Testing Loss:36.525902, Acc:0.760000.\n",
            "\n",
            "people:8, Testing Loss:83.449739, Acc:0.520000.\n",
            "\n",
            "people:9, Testing Loss:39.344225, Acc:0.680851.\n",
            "\n",
            "people:0, Testing Loss:100.745202, Acc:0.600451.\n",
            "\n",
            "#############################\n",
            "epoch 51\n",
            "**********\n",
            "Finish 51 epoch, Loss: 1.162827, Acc: 0.829155\n",
            "Validation Loss: 1.177727, Acc: 0.580000\n",
            "\n",
            "epoch 52\n",
            "**********\n",
            "Finish 52 epoch, Loss: 1.196519, Acc: 0.815743\n",
            "Validation Loss: 1.121703, Acc: 0.592500\n",
            "\n",
            "epoch 53\n",
            "**********\n",
            "Finish 53 epoch, Loss: 1.066556, Acc: 0.851312\n",
            "Validation Loss: 1.169527, Acc: 0.587500\n",
            "\n",
            "epoch 54\n",
            "**********\n",
            "Finish 54 epoch, Loss: 1.065097, Acc: 0.846064\n",
            "Validation Loss: 1.265097, Acc: 0.590000\n",
            "\n",
            "epoch 55\n",
            "**********\n",
            "Finish 55 epoch, Loss: 1.141030, Acc: 0.830904\n",
            "Validation Loss: 1.206856, Acc: 0.595000\n",
            "\n",
            "epoch 56\n",
            "**********\n",
            "Finish 56 epoch, Loss: 0.989362, Acc: 0.853061\n",
            "Validation Loss: 1.272151, Acc: 0.575000\n",
            "\n",
            "epoch 57\n",
            "**********\n",
            "Finish 57 epoch, Loss: 0.984597, Acc: 0.858309\n",
            "Validation Loss: 1.256499, Acc: 0.572500\n",
            "\n",
            "epoch 58\n",
            "**********\n",
            "Finish 58 epoch, Loss: 0.988523, Acc: 0.855394\n",
            "Validation Loss: 1.198640, Acc: 0.612500\n",
            "\n",
            "epoch 59\n",
            "**********\n",
            "Finish 59 epoch, Loss: 0.885549, Acc: 0.872886\n",
            "Validation Loss: 1.212893, Acc: 0.580000\n",
            "\n",
            "epoch 60\n",
            "**********\n",
            "Finish 60 epoch, Loss: 0.902208, Acc: 0.853644\n",
            "Validation Loss: 1.217420, Acc: 0.597500\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:90.054071, Acc:0.480000.\n",
            "\n",
            "people:2, Testing Loss:70.645541, Acc:0.500000.\n",
            "\n",
            "people:3, Testing Loss:53.017414, Acc:0.640000.\n",
            "\n",
            "people:4, Testing Loss:47.615546, Acc:0.620000.\n",
            "\n",
            "people:5, Testing Loss:38.535823, Acc:0.702128.\n",
            "\n",
            "people:6, Testing Loss:69.528788, Acc:0.530612.\n",
            "\n",
            "people:7, Testing Loss:44.001099, Acc:0.720000.\n",
            "\n",
            "people:8, Testing Loss:90.900582, Acc:0.620000.\n",
            "\n",
            "people:9, Testing Loss:42.971827, Acc:0.702128.\n",
            "\n",
            "people:0, Testing Loss:109.432069, Acc:0.611738.\n",
            "\n",
            "#############################\n",
            "epoch 61\n",
            "**********\n",
            "Finish 61 epoch, Loss: 0.894259, Acc: 0.871720\n",
            "Validation Loss: 1.189579, Acc: 0.597500\n",
            "\n",
            "epoch 62\n",
            "**********\n",
            "Finish 62 epoch, Loss: 0.839999, Acc: 0.889796\n",
            "Validation Loss: 1.296587, Acc: 0.607500\n",
            "\n",
            "epoch 63\n",
            "**********\n",
            "Finish 63 epoch, Loss: 0.802970, Acc: 0.885714\n",
            "Validation Loss: 1.304982, Acc: 0.585000\n",
            "\n",
            "epoch 64\n",
            "**********\n",
            "Finish 64 epoch, Loss: 0.807051, Acc: 0.878717\n",
            "Validation Loss: 1.276911, Acc: 0.597500\n",
            "\n",
            "epoch 65\n",
            "**********\n",
            "Finish 65 epoch, Loss: 0.874968, Acc: 0.864140\n",
            "Validation Loss: 1.331359, Acc: 0.585000\n",
            "\n",
            "epoch 66\n",
            "**********\n",
            "Finish 66 epoch, Loss: 0.815241, Acc: 0.871137\n",
            "Validation Loss: 1.304440, Acc: 0.605000\n",
            "\n",
            "epoch 67\n",
            "**********\n",
            "Finish 67 epoch, Loss: 0.775370, Acc: 0.893294\n",
            "Validation Loss: 1.262667, Acc: 0.615000\n",
            "\n",
            "epoch 68\n",
            "**********\n",
            "Finish 68 epoch, Loss: 0.814836, Acc: 0.884548\n",
            "Validation Loss: 1.338571, Acc: 0.597500\n",
            "\n",
            "epoch 69\n",
            "**********\n",
            "Finish 69 epoch, Loss: 0.722506, Acc: 0.896793\n",
            "Validation Loss: 1.418940, Acc: 0.592500\n",
            "\n",
            "epoch 70\n",
            "**********\n",
            "Finish 70 epoch, Loss: 0.897704, Acc: 0.872303\n",
            "Validation Loss: 1.387439, Acc: 0.582500\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:90.391725, Acc:0.480000.\n",
            "\n",
            "people:2, Testing Loss:67.344403, Acc:0.540000.\n",
            "\n",
            "people:3, Testing Loss:62.413824, Acc:0.560000.\n",
            "\n",
            "people:4, Testing Loss:56.718242, Acc:0.580000.\n",
            "\n",
            "people:5, Testing Loss:42.690264, Acc:0.744681.\n",
            "\n",
            "people:6, Testing Loss:67.604450, Acc:0.632653.\n",
            "\n",
            "people:7, Testing Loss:56.118309, Acc:0.600000.\n",
            "\n",
            "people:8, Testing Loss:88.084650, Acc:0.560000.\n",
            "\n",
            "people:9, Testing Loss:41.820245, Acc:0.744681.\n",
            "\n",
            "people:0, Testing Loss:114.649296, Acc:0.602709.\n",
            "\n",
            "#############################\n",
            "epoch 71\n",
            "**********\n",
            "Finish 71 epoch, Loss: 0.877806, Acc: 0.881050\n",
            "Validation Loss: 1.348221, Acc: 0.570000\n",
            "\n",
            "epoch 72\n",
            "**********\n",
            "Finish 72 epoch, Loss: 0.772442, Acc: 0.891545\n",
            "Validation Loss: 1.411966, Acc: 0.570000\n",
            "\n",
            "epoch 73\n",
            "**********\n",
            "Finish 73 epoch, Loss: 0.818206, Acc: 0.881050\n",
            "Validation Loss: 1.310203, Acc: 0.610000\n",
            "\n",
            "epoch 74\n",
            "**********\n",
            "Finish 74 epoch, Loss: 0.667524, Acc: 0.905539\n",
            "Validation Loss: 1.368078, Acc: 0.590000\n",
            "\n",
            "epoch 75\n",
            "**********\n",
            "Finish 75 epoch, Loss: 0.726305, Acc: 0.896793\n",
            "Validation Loss: 1.437941, Acc: 0.587500\n",
            "\n",
            "epoch 76\n",
            "**********\n",
            "Finish 76 epoch, Loss: 0.636057, Acc: 0.914869\n",
            "Validation Loss: 1.374089, Acc: 0.597500\n",
            "\n",
            "epoch 77\n",
            "**********\n",
            "Finish 77 epoch, Loss: 0.644070, Acc: 0.910787\n",
            "Validation Loss: 1.419299, Acc: 0.595000\n",
            "\n",
            "epoch 78\n",
            "**********\n",
            "Finish 78 epoch, Loss: 0.611588, Acc: 0.917784\n",
            "Validation Loss: 1.530258, Acc: 0.587500\n",
            "\n",
            "epoch 79\n",
            "**********\n",
            "Finish 79 epoch, Loss: 0.627544, Acc: 0.917784\n",
            "Validation Loss: 1.473040, Acc: 0.605000\n",
            "\n",
            "epoch 80\n",
            "**********\n",
            "Finish 80 epoch, Loss: 0.632701, Acc: 0.915452\n",
            "Validation Loss: 1.435236, Acc: 0.590000\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:89.379847, Acc:0.520000.\n",
            "\n",
            "people:2, Testing Loss:65.364760, Acc:0.520000.\n",
            "\n",
            "people:3, Testing Loss:69.085562, Acc:0.620000.\n",
            "\n",
            "people:4, Testing Loss:53.922999, Acc:0.660000.\n",
            "\n",
            "people:5, Testing Loss:36.992732, Acc:0.744681.\n",
            "\n",
            "people:6, Testing Loss:82.467536, Acc:0.530612.\n",
            "\n",
            "people:7, Testing Loss:49.684271, Acc:0.640000.\n",
            "\n",
            "people:8, Testing Loss:96.886730, Acc:0.580000.\n",
            "\n",
            "people:9, Testing Loss:47.822597, Acc:0.680851.\n",
            "\n",
            "people:0, Testing Loss:118.292495, Acc:0.609481.\n",
            "\n",
            "#############################\n",
            "epoch 81\n",
            "**********\n",
            "Finish 81 epoch, Loss: 0.664447, Acc: 0.909038\n",
            "Validation Loss: 1.469021, Acc: 0.580000\n",
            "\n",
            "epoch 82\n",
            "**********\n",
            "Finish 82 epoch, Loss: 0.662336, Acc: 0.904373\n",
            "Validation Loss: 1.441347, Acc: 0.575000\n",
            "\n",
            "epoch 83\n",
            "**********\n",
            "Finish 83 epoch, Loss: 0.557494, Acc: 0.911370\n",
            "Validation Loss: 1.521244, Acc: 0.582500\n",
            "\n",
            "epoch 84\n",
            "**********\n",
            "Finish 84 epoch, Loss: 0.598816, Acc: 0.919534\n",
            "Validation Loss: 1.454749, Acc: 0.577500\n",
            "\n",
            "epoch 85\n",
            "**********\n",
            "Finish 85 epoch, Loss: 0.497155, Acc: 0.925948\n",
            "Validation Loss: 1.645593, Acc: 0.592500\n",
            "\n",
            "epoch 86\n",
            "**********\n",
            "Finish 86 epoch, Loss: 0.481535, Acc: 0.935860\n",
            "Validation Loss: 1.640441, Acc: 0.597500\n",
            "\n",
            "epoch 87\n",
            "**********\n",
            "Finish 87 epoch, Loss: 0.566219, Acc: 0.899708\n",
            "Validation Loss: 1.565524, Acc: 0.570000\n",
            "\n",
            "epoch 88\n",
            "**********\n",
            "Finish 88 epoch, Loss: 0.762854, Acc: 0.895627\n",
            "Validation Loss: 1.454042, Acc: 0.595000\n",
            "\n",
            "epoch 89\n",
            "**********\n",
            "Finish 89 epoch, Loss: 0.783413, Acc: 0.883382\n",
            "Validation Loss: 1.398183, Acc: 0.605000\n",
            "\n",
            "epoch 90\n",
            "**********\n",
            "Finish 90 epoch, Loss: 0.746663, Acc: 0.902624\n",
            "Validation Loss: 1.479577, Acc: 0.587500\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:88.370353, Acc:0.540000.\n",
            "\n",
            "people:2, Testing Loss:73.690277, Acc:0.520000.\n",
            "\n",
            "people:3, Testing Loss:84.859043, Acc:0.600000.\n",
            "\n",
            "people:4, Testing Loss:56.326920, Acc:0.620000.\n",
            "\n",
            "people:5, Testing Loss:38.838474, Acc:0.723404.\n",
            "\n",
            "people:6, Testing Loss:87.062326, Acc:0.591837.\n",
            "\n",
            "people:7, Testing Loss:50.586402, Acc:0.640000.\n",
            "\n",
            "people:8, Testing Loss:98.651087, Acc:0.560000.\n",
            "\n",
            "people:9, Testing Loss:39.519855, Acc:0.744681.\n",
            "\n",
            "people:0, Testing Loss:123.505790, Acc:0.613995.\n",
            "\n",
            "#############################\n",
            "epoch 91\n",
            "**********\n",
            "Finish 91 epoch, Loss: 0.568520, Acc: 0.914286\n",
            "Validation Loss: 1.473471, Acc: 0.592500\n",
            "\n",
            "epoch 92\n",
            "**********\n",
            "Finish 92 epoch, Loss: 0.535747, Acc: 0.927697\n",
            "Validation Loss: 1.550863, Acc: 0.587500\n",
            "\n",
            "epoch 93\n",
            "**********\n",
            "Finish 93 epoch, Loss: 0.549397, Acc: 0.913120\n",
            "Validation Loss: 1.502198, Acc: 0.592500\n",
            "\n",
            "epoch 94\n",
            "**********\n",
            "Finish 94 epoch, Loss: 0.503153, Acc: 0.921866\n",
            "Validation Loss: 1.698197, Acc: 0.567500\n",
            "\n",
            "epoch 95\n",
            "**********\n",
            "Finish 95 epoch, Loss: 0.675975, Acc: 0.899708\n",
            "Validation Loss: 1.477957, Acc: 0.582500\n",
            "\n",
            "epoch 96\n",
            "**********\n",
            "Finish 96 epoch, Loss: 0.589384, Acc: 0.917784\n",
            "Validation Loss: 1.538318, Acc: 0.590000\n",
            "\n",
            "epoch 97\n",
            "**********\n",
            "Finish 97 epoch, Loss: 0.607484, Acc: 0.914869\n",
            "Validation Loss: 1.532304, Acc: 0.587500\n",
            "\n",
            "epoch 98\n",
            "**********\n",
            "Finish 98 epoch, Loss: 0.538258, Acc: 0.923032\n",
            "Validation Loss: 1.457670, Acc: 0.592500\n",
            "\n",
            "epoch 99\n",
            "**********\n",
            "Finish 99 epoch, Loss: 0.572156, Acc: 0.927114\n",
            "Validation Loss: 1.459873, Acc: 0.605000\n",
            "\n",
            "epoch 100\n",
            "**********\n",
            "Finish 100 epoch, Loss: 0.521122, Acc: 0.922449\n",
            "Validation Loss: 1.411883, Acc: 0.625000\n",
            "\n",
            "#############################\n",
            "people:1, Testing Loss:107.514584, Acc:0.480000.\n",
            "\n",
            "people:2, Testing Loss:77.545679, Acc:0.520000.\n",
            "\n",
            "people:3, Testing Loss:75.966197, Acc:0.600000.\n",
            "\n",
            "people:4, Testing Loss:48.526365, Acc:0.660000.\n",
            "\n",
            "people:5, Testing Loss:44.864451, Acc:0.723404.\n",
            "\n",
            "people:6, Testing Loss:74.422171, Acc:0.632653.\n",
            "\n",
            "people:7, Testing Loss:54.800498, Acc:0.700000.\n",
            "\n",
            "people:8, Testing Loss:108.954120, Acc:0.580000.\n",
            "\n",
            "people:9, Testing Loss:42.119263, Acc:0.723404.\n",
            "\n",
            "people:0, Testing Loss:126.782235, Acc:0.623025.\n",
            "\n",
            "#############################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2wbZH-C4p-Mt",
        "colab_type": "code",
        "outputId": "127db61e-3df6-4912-ed75-30f925d7a3ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(use_gpu)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kUnur3qkp-Mx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}